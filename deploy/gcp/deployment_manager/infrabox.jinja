{% set CLUSTER_NAME = env['deployment'] %}
{% set SA_NAME = env['deployment'] + '-sa' %}
{% set ZONE = properties['zone'] %}
{% set PROJECT = env['project'] %}
{% set BUCKET = env['project'] + '-' + env['deployment'] + '-config' %}

resources:

- name: {{ CLUSTER_NAME }}
  type: container.v1.cluster
  properties:
    zone: {{ ZONE }}
    cluster:
      name: {{ CLUSTER_NAME }}
      initialClusterVersion: 1.9.2-gke.1
      legacyAbac:
        enabled: false
      initialNodeCount: {{ properties['initialNodeCount'] }}
      nodeConfig:
        machineType: {{ properties["instanceType"] }}
        oauthScopes:
        - https://www.googleapis.com/auth/logging.write
        - https://www.googleapis.com/auth/monitoring

- type: runtimeconfig.v1beta1.config
  name: {{ CLUSTER_NAME }}-config
  properties:
    config: {{ CLUSTER_NAME }}-config

- type: storage.v1.bucket
  name: {{ BUCKET }}
  properties:

- type: storage.v1.bucket
  name: {{ PROJECT }}-{{ CLUSTER_NAME }}-infrabox
  properties:

- name: {{ SA_NAME }}
  type: iam.v1.serviceAccount
  properties:
    accountId: {{ SA_NAME }}
    displayName: {{ SA_NAME }}

- type: runtimeconfig.v1beta1.waiter
  name: {{ CLUSTER_NAME }}-waiter
  metadata:
    dependsOn:
    - {{ CLUSTER_NAME }}-config
  properties:
    parent: $(ref.{{ CLUSTER_NAME }}-config.name)
    waiter: {{ CLUSTER_NAME }}-waiter
    timeout: 600s
    success:
      cardinality:
        path: /success
        number: 1
    failure:
      cardinality:
        path: /failure
        number: 1

- name: {{ CLUSTER_NAME }}-vm
  type: compute.v1.instance
  metadata:
    dependsOn:
    - {{ CLUSTER_NAME }}
  properties:
    zone: {{ ZONE }}
    machineType: https://www.googleapis.com/compute/v1/projects/{{ PROJECT }}/zones/{{ ZONE }}/machineTypes/{{ properties["instanceType"] }}
    tags:
      items:
      -  infrabox-init
    serviceAccounts:
      - email: "default"
        scopes:
        - https://www.googleapis.com/auth/cloud-platform
        - https://www.googleapis.com/auth/compute
        - https://www.googleapis.com/auth/logging.write
        - https://www.googleapis.com/auth/monitoring
        - https://www.googleapis.com/auth/servicecontrol
        - https://www.googleapis.com/auth/service.management.readonly
        - https://www.googleapis.com/auth/userinfo.email
        - https://www.googleapis.com/auth/devstorage.read_write

    networkInterfaces:
    - network: https://www.googleapis.com/compute/v1/projects/{{ PROJECT }}/global/networks/default
      accessConfigs:
      - name: External NAT
        type: ONE_TO_ONE_NAT
    disks:
    - deviceName: boot
      type: PERSISTENT
      boot: true
      autoDelete: true
      initializeParams:
        diskName: {{ CLUSTER_NAME }}-vm-disk
        sourceImage: https://www.googleapis.com/compute/v1/projects/debian-cloud/global/images/debian-8-jessie-v20170918
    metadata:
      items:
      - key: startup-script
        value: |
          #!/bin/bash -x
          apt-get update && apt-get install -y git curl kubectl python python-yaml
          export HOME=/root
          gcloud components update -q
          gcloud components install beta -q
          gcloud container clusters get-credentials {{ CLUSTER_NAME }} --zone {{ ZONE }}

          # Install helm
          curl -LO https://storage.googleapis.com/kubernetes-helm/helm-v2.7.0-linux-amd64.tar.gz
          tar xvf helm-v2.7.0-linux-amd64.tar.gz
          mv ./linux-amd64/helm /usr/bin/helm
          rm -rf linux-amd64
          rm helm-v2.7.0-linux-amd64.tar.gz

          # Init helm
          export PW=$(gcloud container clusters describe {{ CLUSTER_NAME }} --zone {{ ZONE }} | grep password | awk '{ print $2 }')
          kubectl -n kube-system create sa tiller
          kubectl --username=admin --password=$PW create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
          helm init --service-account tiller
          kubectl rollout status -w deployment/tiller-deploy --namespace=kube-system

          # Create namespaces
          kubectl create ns infrabox-system
          kubectl create ns infrabox-worker

          # Install nginx ingress controller
          helm install \
            -n nginx-ingress-controller \
            --namespace kube-system \
            --set rbac.create=true \
            --set controller.config.proxy-body-size="0" \
            stable/nginx-ingress

          # TLS
          openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls.key -out /tmp/tls.crt -subj "/CN={{ properties['domainName'] }}"
          kubectl create -n infrabox-system secret tls infrabox-tls-certs --key /tmp/tls.key --cert /tmp/tls.crt

          # Postgres
          helm install -n postgres --namespace infrabox-system --set postgresPassword=qweasdzxc1,postgresUser=infrabox,postgresDatabase=infrabox stable/postgresql

          # Create service account key
          gcloud iam service-accounts keys create gcs_sa.json --iam-account {{ SA_NAME }}@{{ PROJECT }}.iam.gserviceaccount.com

          # InfraBox
          git clone https://github.com/SAP/infrabox /tmp/infrabox
          cd /tmp/infrabox
          git checkout {{ properties['infraboxVersion'] }}
          ssh-keygen -N '' -t rsa -f id_rsa
          ssh-keygen -f id_rsa.pub -e -m pem > id_rsa.pem

          admin_pw=$(python -c 'import uuid; print uuid.uuid4()')
          echo $admin_pw > admin_password.txt

          webhook_secret=$(python -c 'import uuid; print uuid.uuid4()')
          echo $webhook_secret > webhook_secret.txt

          cat >install.sh <<EOL
          python /tmp/infrabox/deploy/install.py \\
            --version build_393 \\
            -o /tmp/infrabox-configuration \\
            --general-rsa-public-key ./id_rsa.pem \\
            --general-rsa-private-key ./id_rsa \\
            --general-dont-check-certificates \\
            --root-url https://{{ properties['domainName'] }} \\
            --database postgres \\
            --postgres-host postgres-postgresql.infrabox-system \\
            --postgres-username infrabox \\
            --postgres-database infrabox \\
            --postgres-port 5432 \\
            --postgres-password qweasdzxc1 \\
            --storage gcs \\
            --gcs-service-account-key-file gcs_sa.json \\
            --gcs-bucket {{ PROJECT }}-{{ CLUSTER_NAME }}-infrabox \\
            {% if properties['githubEnabled'] %}
            --github-enabled \\
            --github-client-id {{ properties['githubClientID'] }} \\
            --github-client-secret {{ properties['githubClientSecret'] }} \\
            --github-webhook-secret \$(cat webhook_secret.txt) \\
            --github-login-enabled \\
            {% if properties['githubLoginAllowedOrganizationsEnabled'] %}
            --github-login-allowed-organizations {{ properties['githubLoginAllowedOrganizations'] }} \\
            {% endif %}
            {% else %}
            --account-signup-enabled \\
            {% endif %}
            --docker-registry-admin-username admin \\
            --docker-registry-admin-password \$(cat admin_password.txt)
          EOL
          chmod +x install.sh
          sed -i '/^\s*$/d' install.sh
          cat install.sh

          # Generate helm charts
          ./install.sh

          # Upload configuration
          gsutil cp install.sh gs://{{ BUCKET }}/install.sh
          gsutil cp id_rsa gs://{{ BUCKET }}/id_rsa
          gsutil cp id_rsa.pem gs://{{ BUCKET }}/id_rsa.pem
          gsutil cp webhook_secret.txt gs://{{ BUCKET }}/webhook_secret.txt
          gsutil cp admin_password.txt gs://{{ BUCKET }}/admin_password.txt
          gsutil cp gcs_sa.json gs://{{ BUCKET }}/gcs_sa.json

          # Deploy InfraBox
          cd /tmp/infrabox-configuration/infrabox
          helm install -n infrabox .

          # Done
          gcloud beta runtime-config configs variables set success/{{ CLUSTER_NAME }}-waiter success --config-name $(ref.{{ CLUSTER_NAME }}-config.name)
          # gcloud -q compute instances delete {{ CLUSTER_NAME }}-vm --zone {{ ZONE }}
